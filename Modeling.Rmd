---
title: "Modeling"
author: "aaron till"
date: "5/29/2018"
output: html_document
---
```{r}
#library(randomForest)
#library(caret)
library(glmnet)

```


```{r creating 50% random split for training/testing}

set.seed(1234)

train_indices <- sample(1:nrow(main_data), size = 330212, replace = FALSE) #half
train_data <- slice(main_data, train_indices) 
test_data  <- slice(main_data, -train_indices)


```

```{r missclassification function}

get_misclass <- function(model) {
  y <- predict(model, newdata = test_data, type = "response")
  test <- test_data %>%
    mutate(p_hat = y, pred_MME = p_hat > .5)
  table(test$MME, test$pred_MME)
  confusion_mat <- test %>%
    group_by(MME, pred_MME) %>%
    tally()
  false_pos <- confusion_mat[2, 3]
  false_neg <- confusion_mat[3, 3]
  total_obs <- nrow(test)
  misclassification <- (false_pos + false_neg)/total_obs
  misclassification


}
```




# Lasso Regression summerkill testing

```{r source - http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/}

x <- model.matrix(Summerkill ~ Year + Mean_Surf_Temp + Mean_Surf_Zscore + layer_dif + Season + Schmidt + Variance_After_Ice_30 + Cumulative_Above_0 + V2, train_data) 


y <- ifelse(train_data$Summerkill == 1, 1, 0)

```

increasing lambda 

```{r observing lambdas and making final lasso regression model}
set.seed(1234)


lasso_regression_lambdas <- cv.glmnet(x, y, family = "binomial", alpha = 1) # alpha = 0 ridge, 1 for lasso, between 0 and 1 for elastic 

lasso_regression_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = lasso_regression_lambdas$lambda.1se) # alpha = 0 ridge, 1 for lasso, between 0 and 1 for elastic 
#use lambda.1se instead of lambda.min for simpler but less accurate model

#coef(lasso_regression_model)

``` 



```{r}

lasso_regression_test <- model.matrix(Summerkill ~ Year + Mean_Surf_Temp + Mean_Surf_Zscore + layer_dif + Season + Schmidt + Variance_After_Ice_30 + Cumulative_Above_0 + V2, test_data)



probabilities <- lasso_regression_model %>% predict(newx = lasso_regression_test)
predicted_classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy 
observed_classes <- test_data$Summerkill
mean(predicted_classes == observed_classes)



```


Main question: Why do lambda.min and lambda.1se both give me a 0 when computing effectiveness....what does this mean?  Is this bad or good?
 
 
# Lasso regression Summerkill final

```{r source - }

x_summer <- model.matrix(Summerkill ~ Year + Mean_Surf_Temp + Mean_Surf_Zscore + layer_dif + Season + Schmidt + Variance_After_Ice_30 + Cumulative_Above_0 + V2, main_data) 


y_summer <- ifelse(main_data$Summerkill == 1, 1, 0)

```



```{r observing lambdas and making final lasso regression model}
set.seed(1234)


lasso_regression_lambda_summer <- cv.glmnet(x_summer, y_summer, family = "binomial", alpha = 1) # alpha = 0 ridge, 1 for lasso, between 0 and 1 for elastic 

lasso_regression_model_summer <- glmnet(x_summer, y_summer, family = "binomial", alpha = 1, lambda = lasso_regression_lambda_summer$lambda.1se) # alpha = 0 ridge, 1 for lasso, between 0 and 1 for elastic 
#use lambda.1se instead of lambda.min for simpler but less accurate model

coef(lasso_regression_model_summer)

```

# Lasso Just Winterkill

```{r source - }

x_winter <- model.matrix(Winterkill ~ Year + Mean_Surf_Temp + Mean_Surf_Zscore + layer_dif + Season + Schmidt + Variance_After_Ice_30 + Cumulative_Above_0 + Ice_Duration + V2, main_data) 


y_winter <- ifelse(main_data$Winterkill == 1, 1, 0)

```



```{r observing lambdas and making final lasso regression model}
set.seed(1234)


lasso_regression_lambda_winter <- cv.glmnet(x_winter, y_winter, family = "binomial", alpha = 1) # alpha = 0 ridge, 1 for lasso, between 0 and 1 for elastic 

lasso_regression_model_winter <- glmnet(x_winter, y_winter, family = "binomial", alpha = 1, lambda = lasso_regression_lambda_winter$lambda.1se) # alpha = 0 ridge, 1 for lasso, between 0 and 1 for elastic 
#use lambda.1se instead of lambda.min for simpler but less accurate model

coef(lasso_regression_model_winter)

```

 
# Normal Logistic Regression 

Based on feature selection from lasso - is this adequate? NO, scraps this eventually

  Should i do just summerkill


```{r creating reg_predictions and quartiles}
set.seed(1234)

regression_model <- glm(MME ~ Mean_Surf_Temp + layer_dif, family = 'binomial', data=main_data) # data sample for predictions

reg_predictions <- na.omit(future_data) # NEED TO MAKE STILL
reg_predictions$Prob <- predict(regression_model, na.omit(future_data), type = 'response')
#reg_predictions$MME <- ifelse(reg_predictions$Prob > 0.5, 1, 0) - WILL NEVER APPLY
reg_predictions$quantile <- quantile(reg_predictions$Prob,probs = c(min(reg_predictions$Prob), max(reg_predictions$Prob)), na.rm = TRUE)

```


#Building predictions dataset

```{r}

lasso_summer_predictions <- future_data

lasso_summer_predictions$Summerkill <- 0 # Not sure why I am forced to make this

newx<- model.matrix(Summerkill ~ Year + Mean_Surf_Temp + Mean_Surf_Zscore + layer_dif + Season + Schmidt + Variance_After_Ice_30 + Cumulative_Above_0 + V2, lasso_summer_predictions) #why NEWX has to be a fake model and not just a dataset is unclear)



lasso_summer_predictions$Prob <- as.vector(predict(lasso_regression_model_summer, newx= newx, type = 'response'))


lasso_summer_predictions$quantile <- quantile(lasso_summer_predictions$Prob,probs = c(.01, 25, 75)/100, na.rm = TRUE)

```

 c(min(lasso_summer_predictions$Prob), max(lasso_summer_predictions$Prob)
```{r forecasting}
set.seed(1234)

a <- lasso_summer_predictions$Prob
simulation_log_regress <- rbinom(length(a), 1, prob = a)

lasso_summer_predictions$Summerkill_forecast <- simulation_log_regress

```


Optional:
```{r looking at trend of predictions}

change_MME_freq_future <- glm(Summerkill_forecast ~ Year, family = 'binomial', data=reg_predictions)


summary(change_MME_freq_future) 

```



